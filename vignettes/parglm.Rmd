---
title: "Introduction to the parglm package"
author: "Benjamin Christoffersen"
date: "`r Sys.Date()`"
output: rmarkdown::html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, comment = "#>", fig.width = 7, fig.height = 4)
options(digits = 10)
```

The motivation for the `parglm` package is a parallel version of the `glm` 
function. It solves the iteratively re-weighted least squares using a QR 
decomposition with column pivoting with `DGEQP3` function from LAPACK. 
The computation is done in parallel as in the `bam` function in the `mgcv` package. The 
cost is an additional $O(Mp^2 + p^3)$ where $p$ is the number of 
coefficients and $M$ is the number chunks to be computed in parallel. 
The advantage is that you do not need to compile the package with 
an optimized BLAS or LAPACK which supports multithreading. The package also 
includes a method that computes the Fisher information and then solves the normal
equation as in the `speedglm` package. This is faster but less numerically 
stable.

## Example of computation time

```{r set_params, echo = FALSE}
n <- 1000000L
p <- 50L
n_threads <- 18L
```

Below, we perform estimate a logistic regression with `r n` observations 
and `r p` covariates. We vary the number of cores being used with the
`nthreads` argument to `parglm.control`. The `method` arguments sets which
method is used. `LINPACK` yields the QR method and `FAST` yields a similar 
method as in the `speedglm` package.

```{r sim}
#####
# simulate
n # number of observations
p # number of covariates
set.seed(68024947)
X <- matrix(rnorm(n * p, 1/p, 1/sqrt(p)), n, ncol = p)
df <- data.frame(y = 1/(1 + exp(-(rowSums(X) - 1))) > runif(n), X)
```


```{r run, cache = 1}
#####
# compute and measure time. Setup call to make
library(microbenchmark)
library(speedglm)
library(parglm)
cl <- list(
  quote(microbenchmark), 
  glm      = quote(glm     (y ~ ., binomial(), df)), 
  speedglm = quote(speedglm(y ~ ., family = binomial(), data = df)), 
  times = 11L)
tfunc <- function(method = "LINPACK", nthreads)
  parglm(y ~ ., binomial(), df, control = parglm.control(method = method,
                                                         nthreads = nthreads))
cl <- c(
  cl, lapply(1:n_threads, function(i) bquote(tfunc(nthreads = .(i)))))
names(cl)[5:(5L + n_threads - 1L)] <- paste0("parglm-LINPACK-", 1:n_threads)

cl <- c(
  cl, lapply(1:n_threads, function(i) bquote(tfunc(
    nthreads = .(i), method = "FAST"))))
names(cl)[(5L + n_threads):(5L + 2L * n_threads - 1L)] <- 
  paste0("parglm-FAST-", 1:n_threads)

cl <- as.call(cl)
cl # the call we make

out <- eval(cl)
```

```{r show_out}
s <- summary(out) # result from `microbenchmark`
print(s[, c("expr", "min", "mean", "median", "max")], digits  = 3, 
      row.names	= FALSE) 
```

The plot below shows median run times versus the number of cores. The dashed line 
is the median run time of `glm` and the dotted line is the median run time of 
`speedglm`. We could have used `glm.fit` and `parglm.fit`. This would make the 
relative difference bigger as both call e.g., `model.matrix` and `model.frame` which 
do take some time. To show this point, we first compute how much times this takes and 
then we make the plot. The continuous line is the computation time of `model.matrix` and 
`model.frame`.

```{r loadmicro, echo = FALSE}
library(microbenchmark) # in case the previous chunk is not run
```

```{r show_modmat_time, cache = 1}
modmat_time <- microbenchmark(
  modmat_time = {
    mf <- model.frame(y ~ ., df); model.matrix(terms(mf), mf)
  }, times = 10)
modmat_time # time taken by `model.matrix` and `model.frame`
```

```{r show_run_times}
par(mar = c(4.5, 4.5, .5, .5))
o <- aggregate(time ~ expr, out, median)[, 2] / 10^9
ylim <- range(o, 0); ylim[2] <- ylim[2] + .04 * diff(ylim)

o_linpack <- o[-c(1:2, (n_threads + 3L):length(o))]
o_fast    <- o[-(1:(n_threads + 2L))]

plot(1:n_threads, o_linpack, xlab = "Number of cores", yaxs = "i",
     ylim = ylim, ylab = "Run time", pch = 16)
points(1:n_threads, o_fast, pch = 1)
abline(h = o[1], lty = 2)
abline(h = o[2], lty = 3)
abline(h = median(modmat_time$time) / 10^9, lty = 1)
```

The open circles are the `FAST` method and the other circles are the `LINPACK`
method. Again, the `FAST` method and the `speedglm` package compute the Fisher 
information and then solves the normal equation. 
This is advantages in terms of computation cost but may lead to unstable 
solutions. You can alter the number of observations in each parallel chunk 
with the `block_size` argument of `parglm.control`.

The single threaded performance of `parglm` may be slower when there are more coefficients. 
The cause seems to be the difference between the LAPACK and LINPACK implementation. 
This presumably due to either the QR decomposition method and/or the `qr.qty` method. 
On Windows, the `parglm` do seems slower when build with `Rtools` and the reason 
seems so be the `qr.qty` method in LAPACK, `dormqr`, which is slower then the 
LINPACK method, `dqrsl`. Below is an illustration of the 
computation times on this machine. 

```{r check_qr_qty, cache = 1}
qr1 <- qr(X)
qr2 <- qr(X, LAPACK = TRUE)
microbenchmark::microbenchmark(
  `qr LINPACK`     = qr(X), 
  `qr LAPACK`      = qr(X, LAPACK = TRUE), 
  `qr.qty LINPACK` = qr.qty(qr1, df$y), 
  `qr.qty LAPACK`  = qr.qty(qr2, df$y), 
  times = 11)
```

## Session info

```{r ses_info}
sessionInfo()
```

